# Mediapipe-Anime

## 目标

根据用户的面部动作生成同步的虚拟形象，并生成虚拟摄像头，可以使用web端或其他视频软件调取。
目前仅为初步设计，在视频效果、帧率、硬件资源消耗等各方面有所欠缺，将在后续逐步改进完善。

## 算法思路

**面部捕捉**：使用mediapipe对摄像头捕捉到的视频信息进行处理，获取面部的关键点位置信息（landmarks）。

**数据处理**：设计算法将landmarks坐标信息转换为面部动作参数，包括：

* 头部旋转角度（x, y, z)；
* 眼睛开闭程度（左，右）；
* 瞳孔位置（水平方向）；
* 嘴部开闭程度；

其中眉毛形态、竖直方向的瞳孔位置的算法效果不佳，暂时不予考虑。

**虚拟形象构建**：采用文档中所给的`talking-head-anime-2-demo`模型，将经过处理之后的动作数据传入神经网络，根据给定的人物图片，生成相应的动作形象。

**虚拟摄像头**：使用`Unity Capture`生成虚拟摄像头，并通过`pyvirtualcam`库将视频流传入虚拟摄像头。

## 依赖
`talking-head-anime-2-demo`项目所需的依赖有：
* Python >= 3.8
* PyTorch >= 1.7.1 with CUDA support
* SciPY >= 1.6.0
* Matplotlib >= 3.3.4

其他依赖：
* numpy >= 1.19.2 
* opencv-python >= 4.6.0.66 
* mediapipe >= 0.8.10.1 
* pyvirtualcam >= 0.9.1